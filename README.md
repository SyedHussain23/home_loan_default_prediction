# ğŸ¦ home_loan_default_prediction

This project predicts whether a **home loan applicant will default** using machine learning and multi-source financial data.

It demonstrates a realistic **credit risk modeling pipeline**, including:

* Multi-table data integration
* Missing value handling
* Feature engineering via aggregations
* Imbalanced classification handling
* Model comparison
* Cross-validation

The objective is to classify applicants into:

**0 â†’ Non-Defaulter**
**1 â†’ Defaulter**

---

## ğŸ“Œ Project Overview

* **Problem Type:** Binary Classification
* **Domain:** Banking / Credit Risk Analytics
* **Target Variable:** `TARGET`
* **Dataset Size:** 300k+ applications with multiple relational tables
* **Models Used:** Logistic Regression, LightGBM

---

## ğŸ“Š Dataset

The dataset contains multiple relational tables describing customer credit history.

### Tables Used

* application_train.csv
* bureau.csv
* bureau_balance.csv
* credit_card_balance.csv
* POS_CASH_balance.csv
* installments_payments.csv
* previous_application.csv

Dataset Link:
[https://d3ilbtxij3aepc.cloudfront.net/projects/CDS-Capstone-Projects/PRCP-1006-HomeLoanDef.zip](https://d3ilbtxij3aepc.cloudfront.net/projects/CDS-Capstone-Projects/PRCP-1006-HomeLoanDef.zip)

---

## ğŸ” Data Preprocessing

### Missing Value Handling

* Dropped columns with >50% missing values
* Median imputation for numerical features
* Mode imputation for categorical features

### Categorical Encoding

* One-hot encoding applied to categorical variables

### Data Integration

Aggregated relational tables using `groupby + aggregation` to enrich application data.

Examples:

* Bureau credit statistics
* Previous loan amounts and annuities

---

## âš™ï¸ Feature Engineering

* Aggregated credit history features
* Previous loan behavior indicators
* Handling infinite values
* Column name normalization
* Final dataset cleaned with median imputation

---

## âš–ï¸ Imbalanced Data Handling

The dataset contains significantly fewer defaulters.

Solution:

* Applied **scale_pos_weight** in LightGBM
* Evaluated using Recall and AUC-ROC instead of accuracy alone

---

## ğŸ§  Model Training

### Models Implemented

* Logistic Regression (baseline)
* LightGBM (gradient boosting)

Training setup:

* Stratified train-test split
* 3-fold Stratified Cross-Validation
* Reduced dataset size for faster experimentation

---

## ğŸ“ˆ Model Evaluation

### Logistic Regression

* Accuracy â‰ˆ **0.92**
* Precision = **0.0**
* Recall = **0.0**
* F1-score = **0.0**
* AUC-ROC â‰ˆ **0.65**

âš ï¸ Failed to detect defaulters

---

### LightGBM

* Accuracy â‰ˆ **0.75**
* Precision â‰ˆ **0.18**
* Recall â‰ˆ **0.58â€“0.61**
* F1-score â‰ˆ **0.27â€“0.28**
* AUC-ROC â‰ˆ **0.74â€“0.75**

âœ… Successfully detects high-risk customers

---

## ğŸ† Best Model

**LightGBM** is recommended for production because:

* Strong recall for defaulters
* Stable AUC-ROC
* Better risk detection despite lower accuracy

Logistic Regression is suitable only as a baseline.

---

## ğŸ“Š Feature Importance

Top features were extracted using LightGBM importance scores to identify key risk indicators influencing default prediction.

---

## âš ï¸ Challenges & Solutions

### High Missing Values

âœ” Dropped heavily missing columns and applied imputation

### Multiple Data Sources

âœ” Aggregated relational datasets using groupby statistics

### Categorical Variables

âœ” One-hot encoding

### Imbalanced Target

âœ” Used scale_pos_weight and focused on recall + AUC

### Long Training Time

âœ” Reduced CV folds and dataset size

---

## ğŸ› ï¸ Tech Stack

| Tool                 | Purpose           |
| -------------------- | ----------------- |
| Python               | Programming       |
| Pandas / NumPy       | Data processing   |
| Scikit-learn         | ML baseline       |
| LightGBM             | Gradient boosting |
| Matplotlib / Seaborn | Visualization     |
| Requests / Zipfile   | Dataset retrieval |
| Jupyter Notebook     | Experimentation   |

---

## ğŸš€ How to Run

```bash
git clone https://github.com/SyedHussain23/home_loan_default_prediction
cd home_loan_default_prediction
pip install -r requirements.txt
jupyter notebook home_loan_default_prediction.ipynb
```

---

## ğŸ”® Future Improvements

* Hyperparameter tuning
* SMOTE / advanced imbalance handling
* Feature selection techniques
* SHAP explainability
* Model deployment (API)
* Ensemble stacking

---

## ğŸ‘¨â€ğŸ’» Author

**Syed Hussain Abdul Hakeem**

* LinkedIn: [https://www.linkedin.com/in/syed-hussain-abdul-hakeem](https://www.linkedin.com/in/syed-hussain-abdul-hakeem)
* GitHub: [https://github.com/SyedHussain23](https://github.com/SyedHussain23)

---

## â­ Show Your Support

If you found this project useful, consider giving it a â­.
